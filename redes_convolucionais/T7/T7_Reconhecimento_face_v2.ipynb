{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "63JwJ4zsEwnc"
   },
   "source": [
    "# Trabalho #7 - Reconhecimento de face\n",
    "\n",
    "Nesse trabalho você vai criar um sistema de reconhecimento de face. O método utilizado nesse trabalho é da referência [FaceNet](https://arxiv.org/pdf/1503.03832.pdf) que usa a função de custo tripla. Outro método visto em aula é o da referência [DeepFace](https://research.fb.com/wp-content/uploads/2016/11/deepface-closing-the-gap-to-human-level-performance-in-face-verification.pdf), que usa classificação binária. \n",
    "\n",
    "Os problemas de reconhecimento de rosto geralmente se enquadram em duas categorias:\n",
    "\n",
    "- **Verificação de rosto** - \"essa é a pessoa que diz ser?\" Por exemplo, em alguns aeroportos, você pode passar pela alfândega deixando um sistema escanear seu passaporte e depois verificar se você (a pessoa que carrega o passaporte) é a pessoa correta. Um celular que desbloqueia usando seu rosto também está usando verificação de rosto. Este é um problema de correspondência 1 para 1.\n",
    "- **Reconhecimento Facial** - \"quem é essa pessoa?\" Por exemplo, o vídeo mostrado na aula (https://www.youtube.com/watch?v=wr4rx0Spihs) de funcionários da Baidu que entraram no escritório sendo identificados pela face. Este é um problema de correspondência 1 para N.\n",
    "\n",
    "A FaceNet é uma rede neural que codifica uma imagem de rosto em um vetor de 128 números. Ao comparar dois desses vetores, é possível determinar se duas imagens são da mesma pessoa.\n",
    "\n",
    "**Nesta tarefa, você irá:**\n",
    "- Implementar a função de custo tripla;\n",
    "- Usar um modelo pré-treinado para mapear (codificar) imagens de rostos em vetores de 128 elementos;\n",
    "- Usar essas codificações para executar a verificação e reconhecimento de faces.\n",
    "\n",
    "Neste trabalho, usaremos um modelo pré-treinado que representa as ativações das camadas convolucionais usando a convenção do primeiro eixo ser dos canais (filtros), em oposição à convenção do último eixo ser dos canais como usada nas aulas e nos trabalhos anteriores. Em outras palavras, um lote de imagens tem a forma $(m, n_C, n_H, n_W)$ em vez de $(m, n_H, n_W, n_C)$. Ambas as convenções têm uma quantidade razoável de aplicações entre implementações de código aberto, sendo que ainda não existe um padrão uniforme na área de deep learning.\n",
    "\n",
    "Esse trabalho é adaptado de Andrew Ng (deeplearning.ai)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-oqgcNZDEwne"
   },
   "source": [
    "## Coloque os nomes e RAs dos alunos que fizeram esse trabalho\n",
    "\n",
    "Nome e número dos alunos da equipe:\n",
    "\n",
    "Aluno 1:\n",
    "\n",
    "Aluno 2: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arquivos e subdiretórios necessários\n",
    "\n",
    "Nesse trabalho você precisa de dversoss arquivos:\n",
    "\n",
    "1) Diretório com imagens\n",
    "\n",
    "https://drive.google.com/drive/folders/1B7sIDnrH0ZpiWvNzmjUTnwblfrOlM-7Z?usp=sharing\n",
    "\n",
    "2) Diretório com arquivos de parãmetros da rede\n",
    "\n",
    "https://drive.google.com/drive/folders/11bAqlL2QEiYepSFIZt70b9g2YWFram3h?usp=sharing\n",
    "\n",
    "3) Diretório com base de dados\n",
    "\n",
    "https://drive.google.com/drive/folders/1uGv0eNo7pUJQjR5W3imxoE8Tto2o24aG?usp=sharing\n",
    "\n",
    "Faça o download desses diretórios e coloque-os com o mesmo nome no diretório onde estão os arquivos fr_util.py e inception_blocks_v2.py.\n",
    "\n",
    "Observa-se que a base de dados não é necessária para esse trabalho. Ela é necessária se você quiser retreinar a rede."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uTPbOyLyEwnf"
   },
   "source": [
    "## Bibliotecas\n",
    "\n",
    "Execute as células abaixo para importar as bibliotecas necessárias para o trabalho."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "colab_type": "code",
    "id": "OOPpqxk6Ewnm",
    "outputId": "6fe47eea-9b96-49f0-a760-6f92ee097656"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "lvPzrdwlEwnr",
    "outputId": "9404b354-fcfa-48ab-ca18-c5f4470deac6"
   },
   "outputs": [],
   "source": [
    "%cd /content/drive/My Drive/\"Nome do diretório onde estão os arquivos e diretórios\"\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S4PT-cREEwnv"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import MaxPooling2D, AveragePooling2D\n",
    "from tensorflow.keras.layers import Lambda, Flatten, Dense\n",
    "from tensorflow.keras.initializers import glorot_uniform\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras import backend as K\n",
    "K.set_image_data_format('channels_first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6hrS26lVEwn0"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import pandas as pd\n",
    "from fr_utils import *\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DhDpj-oYEwn4"
   },
   "source": [
    "## 1 - Verificação de rosto ingênua\n",
    "\n",
    "Na Verificação de rosto, você recebe duas imagens e precisa dizer se elas são da mesma pessoa. A maneira mais simples de tentar fazer isso seria comparar as duas imagens pixel por pixel. Se a distância entre as imagens for menor que o limite escolhido, pode ser a mesma pessoa!\n",
    "\n",
    "<img src=\"images/pixel_comparison.png\" style=\"width:380px;height:150px;\">\n",
    "<caption><center> <font color='purple'> Figura 1 </center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "WV_CbCvFEwn5"
   },
   "source": [
    "Obviamente, esse algoritmo apresenta um desempenho muito ruim, pois os valores dos pixels mudam drasticamente devido a variações na iluminação, orientação do rosto da pessoa, alterações na posição da cabeça e assim por diante.\n",
    "\n",
    "Vimos em aula que em vez de usar a imagem não processada, utiliza-se uma codificação $f(img)$ da imagem para que as comparações entre elementos dessa codificação forneçam julgamentos mais precisos sobre se duas imagens são da mesma pessoa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oHxoGGVKEwn6"
   },
   "source": [
    "## 2 - Codificando imagens de rosto em um vetor de 128 elementos\n",
    "\n",
    "### 2.1 - Uso de uma rede convolucional para calcular as codificações\n",
    "\n",
    "A rede FaceNet necessita de muitos dados e muito tempo para ser treinada. Portanto, seguindo a prática comum em desenvolvimento de redes deep learning, vamos utilizar uma rede pré-treinada. A arquitetura dessa rede segue o modelo de  [Szegedy et al.] (Https://arxiv.org/abs/1409.4842). Essa rede é do tipo \"inception\" e está configurada no arquivo `inception_blocks.py`. Dê um olhada no arquivo para para ver como ela é implementada.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G8WgFbv2Ewn7"
   },
   "source": [
    "As principais coisas que você precisa saber a rede FaceNet são:\n",
    "\n",
    "- Esta rede usa imagens RGB com resolução de 96x96x3 pixels como entrada. Especificamente, a entrada é uma imagem de rosto (ou um lote de $m$ imagens de rostos) na forma de um tensor de dimensão $(m, n_C, n_H, n_W) = (m, 3, 96, 96)$;\n",
    "- A saída dessa rede é uma matriz de dimensão $(m, 128)$ que codifica cada imagem de face em um vetor de 128 elementos.\n",
    "\n",
    "Execute a célula abaixo para criar a rede que codifica imagens de rosto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "LpUyGlSfEwn8",
    "outputId": "361dc966-016c-459c-ca75-1b3274b63856"
   },
   "outputs": [],
   "source": [
    "from inception_blocks_v2 import *\n",
    "FRmodel = faceRecoModel(input_shape=(3, 96, 96))\n",
    "FRmodel.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AJ6G3ABMEwoA"
   },
   "source": [
    "Note que essa rede possui cerca de 3,7 milhões de parâmetros.\n",
    "\n",
    "Analise a configuração dessa rede e observe os seguintes pontos:\n",
    "\n",
    "- As camadas convolucionais realizam o processo de \"batch normalization\";\n",
    "- Essa rede possui vários módulos \"inceptions\";\n",
    "- A última camada da rede é uma camada densa com 128 unidades, responsável por gerar na saída da rede um vetro de 128 elementos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CS1uLUJTEwoB"
   },
   "source": [
    "Ao usar uma camada totalmente conectada de 128 neurônios como última camada, a rede garante que a saída seja um vetor de codificação de 128 elementos. Esse vetor é então usado para comparar duas imagens de rosto. A Figura 2 mostra o processo utilizado para comparar duas imagens de rosto.\n",
    "\n",
    "<img src=\"images/distance_RNA.png\" style=\"width:680px;height:250px;\">\n",
    "<caption><center> <font color='purple'> Figura 2 <br> <font color='purple'> </center></caption>\n",
    "    \n",
    "Ao calcular a distância entre duas codificações e comparando com um limar, pode-se determinar se as duas imagens representam a mesma pessoa. \n",
    "\n",
    "Portanto, uma codificação é boa se:\n",
    "- As codificações de duas imagens de um amesma pessoa são bastante semelhantes entre si;\n",
    "- As codificações de duas imagens de pessoas diferentes são muito diferentes.\n",
    "\n",
    "A função de custo tripla formaliza esse processo ao \"aproximar\" as codificações de duas imagens da mesma pessoa (referência e positiva) e \"separar\" as codificações de duas imagens de pessoas diferentes (referência e negativa).\n",
    "\n",
    "<img src=\"images/triplet_comparison.png\" style=\"width:280px;height:150px;\">\n",
    "<br>\n",
    "<caption><center> <font color='purple'> Figura 3 <br> <font color='purple'> </center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eYtPOW-xEwoC"
   },
   "source": [
    "### 2.2 - Função de custo tripla\n",
    "\n",
    "Para uma imagem $X$, denotamos a sua codificação por $f(X)$, onde $f$ é uma função calculada pela rede neural.\n",
    "\n",
    "<img src=\"images/f_x.png\" style=\"width:380px;height:150px;\">\n",
    "<caption><center> <font color='purple'> Figura 4 <br>  <font color='purple'> </center></caption>\n",
    "\n",
    "\n",
    "Após a codificação realizada pela rede neural, os vetores $f(X)$ são normalizados para ter norma igual a 1, ou seja:\n",
    "\n",
    "$$\\mid \\mid f(x) \\mid \\mid_2 = 1$$\n",
    "\n",
    "\n",
    "O treinamemnto da rede usa três imagens $(R, P, N)$:  \n",
    "\n",
    "- $R$ é a imagem de \"Referência\" de uma pessoa;\n",
    "- $P$ é a imagem \"Positiva\", ou seja, uma imagem da mesma pessoa da imagem de \"Referência\";\n",
    "- $N$ é a imagem \"Negativa\", ou seja, uma imagem de uma pessoa diferente da imagem de \"Referência\".\n",
    "\n",
    "Esses conjuntos de 3 imagens (triplas) são escolhidos dos exemplos de treinamento. Usaremos a notação $(R^{(i)}, P^{(i)}, N^{(i)})$ para representar o $i$-ésimo exemplo de treinamento, que consiste de 3 imagens.\n",
    "\n",
    "Deseja-se garantir que a imagem $R ^{(i)}$ de um indivíduo esteja mais próxima da imagem positiva $P^{(i)}$ do que da imagem negativa $N^{(i)}$ por pelo menos uma margem $\\alpha$. Esse critério é descrito pela seguinte equação:\n",
    "\n",
    "$$\\mid\\mid f(R^{(i)}) - f(P^{(i)}) \\mid\\mid_2^2 + \\alpha < \\mid\\mid f(R^{(i)}) - f(N^{(i)}) \\mid\\mid_2^2$$\n",
    "\n",
    "Assim é desejado minimizar a seguinte fução de custo tripl:\n",
    "\n",
    "$$\\mathcal{J} = \\sum^{m}_{i=1} max \\large[ \\small \\underbrace{\\mid \\mid f(R^{(i)}) - f(P^{(i)}) \\mid \\mid_2^2}_\\text{(1)} - \\underbrace{\\mid \\mid f(R^{(i)}) - f(N^{(i)}) \\mid \\mid_2^2}_\\text{(2)} + \\alpha, 0 \\large]$$\n",
    "\n",
    "Note que o valor da função de custo tripla é menor ou igual a zero, ou seja o seu valor máximo é zero. \n",
    "\n",
    "Observações:\n",
    "- O termo (1) da equação acima é a distância ao quadrado entre a imagm de referência $R$ e a imagem positiva $P$ para uma dada tripla. É desejado que esse valor seja pequeno;\n",
    "- O termo (2) da equação acima é a distância ao quadrado entre a imagem de referência $R$ e a negativa $N$ para uma determinada tripla. É desejado que esse valor seja relativamente grande.\n",
    "- $\\alpha$ é a margem, que consiste de um hiperparâmetro que deve ser escolhido. Nesse trabalho usaremos $\\alpha = 0.2$.\n",
    "\n",
    "A maioria das implementações também normaliza os vetores de codificação para ter norma igual a um (isto é, $\\mid\\mid f(x) \\mid\\mid_2 = 1$). Não usaremos isso nesse trabalho.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E3h60wQyEwoD"
   },
   "source": [
    "### Exercício #1: Implementação da função de custo tripla\n",
    "\n",
    "Os passos para implementar a funçaõ de custo tripla são os seguintes:\n",
    "\n",
    "1. Calcule a distância entre as codificações das imagens de \"referência\" e \"positiva\": $\\mid\\mid f(R^{(i)}) - f(P^{(i)}) \\mid\\mid_2^2$;\n",
    "2. Calcule a distância entre as codificações das imagens de \"referência\" e \"negativa\": $\\mid\\mid f(R^{(i)}) - f(N^{(i)}) \\mid\\mid_2^2$;\n",
    "3. Para cada exemplo treinamento calcule a expressão: $\\mid \\mid f(R^{(i)}) - f(P^{(i)}) \\mid \\mid_2^2 - \\mid \\mid f(R^{(i)}) - f(N^{(i)}) \\mid \\mid_2^2 + \\alpha$;\n",
    "4. Calcule a expressão completa, tomando o máximo com zero e somando para todos os exemplos de treinamento.\n",
    "\n",
    "Funções úteis: `tf.reduce_sum()`, `tf.square()`, `tf.subtract()`, `tf.add()`, `tf.maximum()`.\n",
    "\n",
    "Nos passos 1 e 2, você tem que somar todos os elementos de $\\mid \\mid f(A^{(i)}) - f(P^{(i)}) \\mid \\mid_2^2$ e $\\mid \\mid f(A^{(i)}) - f(N^{(i)}) \\mid \\mid_2^2$, equanto que no passo 4 você tem que somar para todos os exemplos de treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7g2MFEpbEwoE"
   },
   "outputs": [],
   "source": [
    "# PARA VOCÊ FAZER: Função de custo tripla\n",
    "\n",
    "def triplet_loss(y_true, y_pred, alpha = 0.2):\n",
    "    \"\"\"\n",
    "    Argumentos:\n",
    "    y_true = rótulo desejado, exigido quando se define um função de custo no Keras, mas nesse caso não é necessário\n",
    "    y_pred = lista python contendo 3 objetos:\n",
    "             Referência = codificação da imagem de referência, diomensão (None, 128)\n",
    "             Positiva = codificação da imagem positivo, dimensão (None, 128)\n",
    "             Negativa = codificação da imagem negativa, dimensão (None, 128)\n",
    "    \n",
    "    Retorna:\n",
    "    loss = número real que consiste no valor da função de custo tripla\n",
    "    \"\"\"\n",
    "\n",
    "    referencia, positiva, negativa = y_pred[0], y_pred[1], y_pred[2]\n",
    "    \n",
    "    ### COMECE AQUI ### (≈ 4 linhas)\n",
    "    # Passo 1: Calcule a distência entre as codificações da referência e positivathe, necessário somar ao longo de axis=-1\n",
    "    #pos_dist = \n",
    "    # Passo 2: Calcule a distência entre as codificações da referência e negativa, necessário somar ao longo de axis=-1\n",
    "    #neg_dist = \n",
    "    # Passo 3: subtrair as distâncias calculadas nos passos anteriores e somar alpha.\n",
    "    #basic_loss = \n",
    "    # Passo 4: Calcule o máximo entre basic_loss e 0.0 e depois some para todos es exemplos.\n",
    "    #loss = \n",
    "    ### TERMINE AQUI ###\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "77uvxbq7EwoI",
    "outputId": "1ef7fb1c-c555-469c-e5e6-58bfd81776f3"
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(1)\n",
    "y_true = (None, None, None)\n",
    "y_pred = (tf.random.normal([3, 128], mean=6, stddev=0.1, seed = 1),\n",
    "          tf.random.normal([3, 128], mean=1, stddev=1, seed = 1),\n",
    "          tf.random.normal([3, 128], mean=3, stddev=4, seed = 1))\n",
    "loss = triplet_loss(y_true, y_pred)\n",
    "    \n",
    "print(\"loss = \", format(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rSBZzUZLEwoM"
   },
   "source": [
    "**Saída esperada:**\n",
    "\n",
    "    loss =  527.2598266601562"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K4fexRvzEwoN"
   },
   "source": [
    "## 3 - Carregar a rede FaceNet\n",
    "\n",
    "A rede FaceNet é treinada minimizando a perda de custo tripla. Mas como o treinamento exige muitos dados e muita computação, não vamos treiná-la do zero nesse trabalho. Em vez disso, carregamos um modelo treinado anteriormente. \n",
    "\n",
    "Execute a célula abaixo para carregar a FaceNet. isso pode levar alguns minutos para ser executado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XDUMDFC-EwoN"
   },
   "outputs": [],
   "source": [
    "FRmodel.compile(optimizer = 'adam', loss = triplet_loss, metrics = ['accuracy'])\n",
    "load_weights_from_FaceNet(FRmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8NRvmmOmEwoS"
   },
   "source": [
    "Alguns exemplos de distâncias entre as codificações para três indivíduos são mostradas na Figura :\n",
    "\n",
    "<img src=\"images/distance_matrix.png\" style=\"width:380px;height:200px;\"><br>\n",
    "<caption><center> <font color='purple'> Figure 5 (Andrew Ng, deeplearning.ai) <br>  <font color='purple'> </center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4LfIZx-rEwoT"
   },
   "source": [
    "## 4 - Usando a rede FaceNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DSUgMZLvEwoU"
   },
   "source": [
    "### 4.1 - Verificação de face\n",
    "\n",
    "Vamos construir um banco de dados contendo um vetor de codificação para cada pessoa autorizada a acessar um determinado local. Para gerar a codificação, usamos `img_to_encoding (path_da_imagem, rede)`, que basicamente executa a propagação direta da rede FaceNet para a imagem especificada.\n",
    "\n",
    "Execute o código da célula abaixo para criar o banco de dados (representado como um dicionário python). Esse banco de dados mapeia o rosto de cada pessoa para um vetor codificado de 128 elementos e o associa ao nome da pessoa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "5UHpFUnbEwoU"
   },
   "outputs": [],
   "source": [
    "database = {}\n",
    "database[\"joao\"] = img_to_encoding(\"images/joao.jpg\", FRmodel)\n",
    "database[\"antonio\"] = img_to_encoding(\"images/antonio.jpg\", FRmodel)\n",
    "database[\"miguel\"] = img_to_encoding(\"images/miguel.jpg\", FRmodel)\n",
    "database[\"daniel\"] = img_to_encoding(\"images/daniel.jpg\", FRmodel)\n",
    "database[\"jose\"] = img_to_encoding(\"images/jose.jpg\", FRmodel)\n",
    "database[\"pedro\"] = img_to_encoding(\"images/pedro.jpg\", FRmodel)\n",
    "database[\"maria\"] = img_to_encoding(\"images/maria.jpg\", FRmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fn2gaI8lEwoY"
   },
   "source": [
    "Agora, quando alguém quiser acessar o local restrito e passa o cartão de identificação que define o nome, pode-se procurar a codificação no banco de dados e usá-la para verificar se essa pessoa corresponde ao nome na identificação."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IvB5RB21EwoZ"
   },
   "source": [
    "### Exercício #2: Implementação da função verify()\n",
    "\n",
    "A função `verify()` verifica se a imagem do rosto da pessoa querendo acessar o ambiente restrito (`path_da_imagem`) é de fato da pessoa do cartão de identidade apresentado. Para isso fazer essa função, você precisa seguir as seguintes etapas:\n",
    "\n",
    "1. Calcule a codificação da imagem definida em image_path;\n",
    "2. Calcule a distância entre essa codificação e a codificação da imagem da identidade armazenada no banco de dados;\n",
    "3. Libere o acesso se a distância for menor que 0,7, caso contrário, não libere.\n",
    "\n",
    "Nesse cálculo você deve calcular a distância entre os dois vetores de codificação e não o quadrado dessa distância com é feito  na função de custo tripla, ou seja, deve-se calcular a seguinte expressão:\n",
    "\n",
    "$$\\mid\\mid f(R) - f(P) \\mid\\mid_2$$\n",
    "\n",
    "Para fazer esse cálculo utilize as funções numpy `np.linalg.norm` e `np.subtract`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jNu3Gdf0Ewoa"
   },
   "outputs": [],
   "source": [
    "# PARA VOCÊ FAZER: Função verify\n",
    "\n",
    "def verify(image_path, identity, database, model):\n",
    "    \"\"\"\n",
    "    Função que verifica se a pessoa na \"image_path\" é a da \"identidade\".\n",
    "    \n",
    "    Argumentos:\n",
    "    image_path = path para a imagem\n",
    "    identity = string, nome da pessoa que se deseja verificar a identidade\n",
    "    database = dicionário python que mapeia nomes de pessoas em vetores\n",
    "    model = RNA inception\n",
    "    \n",
    "    Returns:\n",
    "    dist =- distance between the image_path and the image of \"identity\" in the database.\n",
    "    door_open -- True, if the door should open. False otherwise.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### COMECE AQUI ###\n",
    "    \n",
    "    # Passo 1: Calcule o vetor de codificação da imagem, use img_to_encoding(), veja exemplo acima (≈ 1 linha)\n",
    "    #encoding = \n",
    "       \n",
    "    # Passo 2: calcula distância entre vetor de codificação da imagem e vetor de codificação da identificação (≈ 1 linha)\n",
    "    #dist = \n",
    "           \n",
    "    # Passo 3: Permite acesso se dist < 0.7, senão bloqueia acesso (≈ 3 linhas)\n",
    "    #if \n",
    "        #print\n",
    "        #door_open = \n",
    "    #else:\n",
    "        #print\n",
    "        #door_open \n",
    "    ### TERMINE AQUI ###\n",
    "        \n",
    "    return dist, door_open"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bKFeBBMmEwoe"
   },
   "source": [
    "João está tentando entrar no ambiente restyrito e uma imagem dele é adquirida pela câmera (\"images/joao02.jpg\"). Execute a célula abaixo com o algoritmo de verificação de face com essa image:\n",
    "\n",
    "<img src=\"images/joao01.jpg\" style=\"width:100px;height:100px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "hl7F6hafEwof",
    "outputId": "763b9312-d39c-499a-8856-c864b3b562e7"
   },
   "outputs": [],
   "source": [
    "verify(\"images/joao02.jpg\", \"joao\", database, FRmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "_dQ9pQZcEwok"
   },
   "source": [
    "**Saída esperada:**\n",
    "\n",
    "    É o joao, seja bem vindo!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "eOZYBWaKEwok"
   },
   "source": [
    "Uma pessoa desconhecida está tentando acessar o ambiente com aidentificação do Pedro. A imagem dessa pessoa é adquirida pela câmera (\"images/daniel.jpg). Execute o algoritmo de verificação de face da célula abaixo para verificar se essa pessoa pode entrar.\n",
    "\n",
    "<img src=\"images/daniel.jpg\" style=\"width:100px;height:100px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "wfFeOXlEEwol",
    "outputId": "944dc981-2b4c-4324-ce37-592342df4ce4"
   },
   "outputs": [],
   "source": [
    "verify(\"images/daniel02.jpg\", \"pedro\", database, FRmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nWkz8Xp_Ewop"
   },
   "source": [
    "**Saída esperada:**\n",
    "\n",
    "    Não é o pedro, por favor se identifique novamente!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Ke6haxqEwor"
   },
   "source": [
    "### 4.2 - Reconhecimento de face\n",
    "\n",
    "Na medida em que o sistema de verificação de face está funcionando corretamente, vamos agora modificá-lo para transformá-lo em um sistema de reconhecimento de face. Em um sistema de reconhecimento de face não é necessário que os usuários utilizem a sua identificação para acessar a área restrita; uma pessoa autorizada simplesmente para em frente a porta e a porta se abre para ela. \n",
    "\n",
    "Você vai implementar um sistema de reconhecimento de face que recebe como entrada uma imagem e decide se essa pessoa está autorizada, ou não, a acessar a área restrita. Diferentemente do sistema de verificação de face, não é necessário fornecer a indentidade da pessoa como outra entrada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wbejoRLUEwor"
   },
   "source": [
    "### Exercício #3: Implementação da função  `who_is_it()`.\n",
    "\n",
    "Para implementar a função de reconhecimento de face `who_is_it()`, você tem que seguir as seguintes etapas:\n",
    "\n",
    "1. Calcule o vetor de codificação da imagem do caminho especificado em `image_path`. Denominaremos esse vetor de vetor alvo.\n",
    "2. Ache o vetor de codificação do banco de dados que tem a menor distância do vetor de codificação da imagem de entrada (vetor alvo). \n",
    "    - Inicialize a variável `min_dist` com um número grande, por exemplo 100. \n",
    "    - Inicialize a variável `identidy` com `None`.\n",
    "    - Percorra o dicionário do banco de dados, que contém os vetores de codificação e os nomes das pessoas associadas. Use o comando `for(name, db_enc) in database.items()`. Observa-se que esse comando é próprio da linguagem Python para buscar itens em um dicionário.\n",
    "    - Percorrendo o banco de dados, calcule a distância entre entre o vetor alvo e cada um dos vetores do banco de dados (`dist`). Se a cada verificação a distância `dist` for menor do que `min_dist`, então substitua `min_dist` por `dist` e `identity` pelo nome da pessoa no banco de dados correspondente ao item sendo comparado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P3gacyqEEwos"
   },
   "outputs": [],
   "source": [
    "# PARA VOCÊ FAZER: Função who_is_it()\n",
    "\n",
    "def who_is_it(image_path, database, model):\n",
    "    \"\"\"\n",
    "    Implementa reconhecimento de face identificando quem é a pessoa em image_path image.\n",
    "    \n",
    "    Argumentos:\n",
    "    image_path = caminho para uma image\n",
    "    database = banco de dados que contém os vetores de codificação das feces e nomes das pessoas associadas\n",
    "    model = RNA FaceNet\n",
    "    \n",
    "    Retorna:\n",
    "    min_dist = distância mínima entre o vetor de codificação da image_path e os vetores de codificação do banco de dados\n",
    "    identity = string com o nome dapessoa em image_path\n",
    "    \"\"\"\n",
    "    \n",
    "    ### COMECE AQUI ### \n",
    "    \n",
    "    ## Passo 1: calcula o vetor de codificação alvo; use img_to_encoding(), veja examplo acima ## (≈ 1 linha)\n",
    "    #encoding = \n",
    "    \n",
    "    ## Passo 2: Acho a codificação de menor distância\n",
    "    \n",
    "    # Inicializa \"min_dist\" com um número grande, , por exemplo 100, e indentity com None (≈ 2 linhas)\n",
    "    min_dist = \n",
    "    identity = \n",
    "    \n",
    "    # Percorra o dicionário do banco de dados com os vetores de codificação e nomes\n",
    "    for (name, db_enc) in database.items():    \n",
    "        \n",
    "        # Calcule a distãncia entre o vetor alvo e o vetor de codificação corrente \"emb\" do banco de dados (≈ 1 linha)\n",
    "        #dist = \n",
    "\n",
    "        # Se a distãncia for menor do que min_dist, então atribua dist em min_dits e o nome em identity to name. (≈ 3 linhas)\n",
    "        #if dist \n",
    "            #min_dist = \n",
    "            #identity = \n",
    "    ### TERMINE AQUI ###\n",
    "    \n",
    "    if min_dist > 0.7:\n",
    "        print(\"Não está na base de dados!\")\n",
    "    else:\n",
    "        print (\"É \" + str(identity) + \", a distância é \" + str(min_dist))\n",
    "        \n",
    "    return min_dist, identity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kX0D8HNTEwov"
   },
   "source": [
    "Younes is at the front-door and the camera takes a picture of him (\"images/camera_0.jpg\"). Let's see if your who_it_is() algorithm identifies Younes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "jykxa7UjEwow",
    "outputId": "0ab3a9e8-0f2b-4e4b-ffb4-69de1e4c9c8c",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "who_is_it(\"images/camera8.jpg\", database, FRmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xNIkA3Y_Ewoz"
   },
   "source": [
    "**Saída esperada:**\n",
    "\n",
    "    É joao, a distância é 0.6593835\n",
    "    \n",
    "    (0.6593835, 'joao')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E8hdlf9wEwo0"
   },
   "source": [
    "### Exercício #4: teste do sistema de reconhecimento de faces\n",
    "\n",
    "Teste o seu programa usando outras imagens, simplesmente alterando a imagem de entrada, como por exemplo, use qualquer imagem com nome `camera+número.jpg` e veja os resultados. Teste pelo menos 5 imagens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "colab_type": "code",
    "id": "fLk14kAeEwo0",
    "outputId": "688178c8-1b3e-4555-eea1-b85e805dce70"
   },
   "outputs": [],
   "source": [
    "# PARA VOCÊ FAZER: teste do sistema de reconhecimento de face\n",
    "\n",
    "### COMECE AQUI ### \n",
    "# Teste o seu sistema de reconhecimento de face (≈ 6 linhas). \n",
    "# Use as imagems camera1.jpg, camera2.jpg, camera3.jpg, camera4.jpg, camera5.jpg, camera6.jpg\n",
    "#\n",
    "### TERMINE AQUI ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FApbqAsrEwo3"
   },
   "source": [
    "**Saída esperada:**\n",
    "\n",
    "    É antonio, a distância é 0.25238562\n",
    "    É maria, a distância é 0.6854024\n",
    "    É joao, a distância é 0.59267086\n",
    "    Não está na base de dados!\n",
    "    Não está na base de dados!\n",
    "    Não está na base de dados!\n",
    "    \n",
    "Observa-se que essa saída é somente um exemplo. Usando outras imagens o resultado será diferente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BEp7UzFOEwo5"
   },
   "source": [
    "### Referências:\n",
    "\n",
    "- Florian Schroff, Dmitry Kalenichenko, James Philbin (2015). [FaceNet: A Unified Embedding for Face Recognition and Clustering](https://arxiv.org/pdf/1503.03832.pdf)\n",
    "- Yaniv Taigman, Ming Yang, Marc'Aurelio Ranzato, Lior Wolf (2014). [DeepFace: Closing the gap to human-level performance in face verification](https://research.fb.com/wp-content/uploads/2016/11/deepface-closing-the-gap-to-human-level-performance-in-face-verification.pdf) \n",
    "- A RNA pré-treinada usada nesse trabalho foi desenvolvida por Victor Sy Wang's e obtida de: https://github.com/iwantooxxoox/Keras-OpenFace.\n",
    "- Repositótio oficial da FaceNet: https://github.com/davidsandberg/facenet \n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "T7_Reconhecimento_face.ipynb",
   "provenance": []
  },
  "coursera": {
   "course_slug": "convolutional-neural-networks",
   "graded_item_id": "IaknP",
   "launcher_item_id": "5UMr4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
